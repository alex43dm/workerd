workerd: multithread demon, to return html(inframe) with offers. Runs on background and listen unix socket(config.xml) ready to http requests from nginx.
 
Input
http variables:
	QUERY_STRING 
	REMOTE_ADDR
	SCRIPT_NAME
	HTTP_COOKIE

and url variables:
	show(status) – show status
	ip(REMOTE_ADDR) - ip
	cookie_id(HTTP_COOKIE) – cookie id
	scr – informer id
	country – country for tests 
	region – region for tests
	show(json) – html or json returns
	location – return to http response
	w - width
	h - height
	search – user search keywords
	context – site context keywords

Outputs:
html with offers selected by rating from:
cat db_dump/requests/01.sql 

SELECT ofrs.id,
ofrs.guid,
ofrs.title,
ofrs.price,
ofrs.description,
ofrs.url,
ofrs.image,
ofrs.swf,
ofrs.campaignId,
ofrs.isOnClick,
ofrs.type,
CASE WHEN iret.rating IS NOT NULL
THEN iret.rating
ELSE ofrs.rating
END AS rating,
ofrs.retargeting,
ofrs.uniqueHits,
ofrs.height,
ofrs.width,
ca.social
FROM Offer AS ofrs
INNER JOIN Campaign AS ca ON ca.valid=1 AND ca.retargeting=0 AND ofrs.campaignId=ca.id
INNER JOIN (
		SELECT cn.id FROM CampaignNow AS cn
		%s
        INNER JOIN Campaign2Categories AS c2c ON cn.id=c2c.id_cam
        INNER JOIN Categories2Domain AS ct2d ON c2c.id_cat=ct2d.id_cat
                AND ct2d.id_dom=%lld
		EXCEPT
            SELECT c2d.id_cam AS id
            FROM Campaign2Domains AS c2d
            WHERE c2d.id_dom=%lld AND c2d.allowed=0
		EXCEPT
            SELECT c2a.id_cam AS id
            FROM Campaign2Accounts AS c2a
            WHERE c2a.id_acc=%lld AND c2a.allowed=0
        EXCEPT
            SELECT c2i.id_cam AS id
            FROM Campaign2Informer AS c2i
            WHERE c2i.id_inf=%lld AND c2i.allowed=0
) AS c ON ca.id=c.id
LEFT JOIN tmp%d%lld AS deph ON ofrs.id=deph.id
LEFT JOIN Informer2OfferRating AS iret ON iret.id_inf=%lld AND ofrs.id=iret.id_ofr
WHERE ofrs.valid=1
    AND deph.id IS NULL
;

where:
tmp%d%lld – user view offers history
CampaignNow - campaign allowed now by time
%s - 
IF:

SELECT geo.id_cam FROM geoTargeting AS geo \
                INNER JOIN GeoLiteCity AS reg ON geo.id_geo = reg.locId AND reg.city='city by geo ip';"

RETURNS COUNT > 0

THEN:

INNER JOIN geoTargeting AS geo ON geo.id_cam=cn.id \
        INNER JOIN GeoLiteCity AS reg ON geo.id_geo = reg.locId AND reg.city='data from geo ip or url request'

ELSE:

INNER JOIN geoTargeting AS geo ON geo.id_cam=cn.id \
            INNER JOIN GeoLiteCity AS reg ON geo.id_geo = reg.locId AND(reg.country='UA' AND reg.city='')
ELSE:
EMPTY STRING

Core algorithm

each thread wait input, so it cann't get cpu resources, after parsing intput variables(class Param) and get geo data:
1) load informer:
cat db_dump/requests/02.sql 
SELECT 	inf.id,
	inf.capacity,
	inf.bannersCss,
	inf.teasersCss,
	inf.domainId,
	inf.accountId
FROM Informer AS inf
WHERE inf.guid=@informerUid
LIMIT 1; 

2) load all histories(class HistoryManager) asynchronous in it own thread.
The HistoryManager get short term, long term, user view, category, re-targeting from redis servers asynchronous.
The shot, long histories and seach, category url parameters use for reting tune by shpinx full text search on offer collection(Outputs) with multiplication coefficients from config.xml:

<range>
    <short_term>0</short_term>
    <long_term>0</long_term>
    <context>0</context>
    <search>1.2</search>
</range>
 if 0 – does not use in sphinx search and the don't make history request.
For fine tune sphinx(Context search by offers)

3) get all offers from request(Outputs)

4) wait until all histories were loaded and process rating correction in sphinx(class XXXSearcher) history with zero rating does not use

5) process RIS algorithm(main)
input:
	const Offer::Map &items,
	unsigned outLen(capacity)
output:
	Offer::Vector &RISResult

a) if result size less then capacity go to expand vector size(j);
b) sort by rating
c) teaser rating and banner check: if fount first banner(max rating) return
d) turn on clean history flag if all offers are social
e) remove social
f) add teaser with company unique and rating > 0 if teasers count more capacity return
g) user history view clean
h) add teaser with company unique and with any rating  if teasers count more capacity return
i) teaser with id unique and with any rating  if teasers count more capacity return
j) expand vector size to capacity

6) wait and process re-targeting offers request by RIS algorithm

7) merge re-targeting and main vector;

8) output html

9) save log and user histories

RUN
workerd -c path/to/config.xml [-s path/to/unix/socket/path]

DEBUG
If the program configured with –debug-enable it does not fork in background, and will print debug information to stdout. 

SYSLOG-NG
add to /etc/syslog-ng/syslog-ng.conf:

template t_isots
{
    template("$R_ISODATE $MSG\n");
    template_escape(no);
};

destination getmyad { file("/var/log/workerd.log" template(t_isots)); };
filter f_getmyad { program("getmyad"); };
log { source(src); filter(f_getmyad); destination(getmyad); };

NIGING
the sample nginx configuration:

        upstream fastcgi_backend {
                server unix:/tmp/workerd.sock;
                server unix:/tmp/workerd1.sock;
                server unix:/tmp/workerd2.sock;
        }

        server {
                listen 0.0.0.0;
                server_name yottos.com;
                access_log off;#/var/log/nginx/localhost.access_log main;
                error_log /var/log/nginx/localhost.error_log info;
                root /var/www/localhost/htdocs;
                keepalive_timeout 1ms;

                location /adshow.fcgi { 
                        send_timeout 100ms;
                        fastcgi_cache off;
                        fastcgi_pass fastcgi_backend;
                        fastcgi_buffer_size 24k;
                        fastcgi_buffers 2 24k;
                        fastcgi_busy_buffers_size 24k;
                        fastcgi_keep_conn on;
                        include fastcgi_params; 
                } 
        }

DATABASES
At start(if not configured with --enable-debug) forks in background and get data from mongo database described in config.xml section:
<mongo>
    <main>
        <!--ip:port-->
        <host>213.186.121.76:27018</host>
        <!--the database name-->
        <db>getmyad_db</db>
        <!--replica set name-->
        <set></set>
       <!-- enable slave-->
        <slave>true</slave>
    </main>
</mongo>

The database will be loaded into sqlite database file(may be file on tmpfs for debug or control proposes, check write access from user):
<dbpath>/run/mem.db</dbpath>
 or memory:
<dbpath>:memory:</dbpath>
The database dump 
db_dump/requests/02.sql
db_dump/requests/01.sql
db_dump/requests/04.sql
db_dump/requests/03.sql
db_dump/UA_RU
db_dump/tables/11_GeoCountries.sql
db_dump/tables/00_Campaign.sql
db_dump/tables/17_Retargeting.sql
db_dump/tables/12_GeoRerions.sql
db_dump/tables/20_GeoLiteCity.sql
db_dump/tables/06_Accounts.sql
db_dump/tables/02_Offer.sql
db_dump/tables/07_Campaign2Accounts.sql
db_dump/tables/15_CampaignNow.sql
db_dump/tables/16_Informer.sql
db_dump/tables/18_Informer2OfferRating.sql
db_dump/tables/10_CronCampaign.sql
db_dump/tables/05_Campaign2Informer.sql
db_dump/tables/14_regionTargeting.sql
db_dump/tables/08_Domains.sql
db_dump/tables/19_Campaign2Categories.sql
db_dump/tables/04_Categories2Domain.sql
db_dump/tables/13_geoTargeting.sql
db_dump/tables/03_Categories.sql
db_dump/tables/09_Campaign2Domains.sql
db_dump/tables/04_Categories2Informer.sql
db_dump/view/00_Campaign2GeoRerions.sql
db_dump/view/03_Campaign2Acnts.sql
db_dump/view/02_Campaign2Doms.sql
db_dump/view/04_Campaign2Infs.sql
db_dump/post/01.sql
db_dump/schema/CampaignNow.sql

loaded from
<db_dump_path>/etc/worker/db_dump</db_dump_path>
all files from sub directories sort by name and load into database.

THREADS
Threads number: server/children

SPHINX
To match search 
<sphinx>
    <!--host>213.186.121.76</host-->
    <host>127.0.0.1</host>
    <port>9312</port>
    <index>worker</index>
    <!--all from /usr/include/sphinxclient.h-->
    <mach_mode>SPH_MATCH_EXTENDED2</mach_mode>
    <rank_mode>SPH_RANK_SPH04</rank_mode>
    <sort_mode>SPH_SORT_RELEVANCE</sort_mode>
    <fields>
        <title>80</title>
        <description>30</description>
        <!--keywords>70</keywords-->
        <!--exactly_phrases>100</exactly_phrases-->
        <!--phrases>90</phrases-->
        <!--minuswords>100</minuswords-->
    </fields>
</sphinx>

ALL
Please read and edit config.xml to fine tune. 

The configuration options by groups(mongo/main/db means xpath:
<mongo><main><db/></main></mongo>):

Main configuration:
	server_ip: ip uses to redirect script
	redirect_script: the redirect script name
	geocity_path: geo city database file path
	shortterm_expire: time to live keys in shortterm redis database in seconds
	views_expire: time to live keys in  user view redis database in seconds
	context_expire: time to live keys in site contex  redis database in seconds

The  re-targeting
	retargeting_by_persents: how match max re-targeting offers can be in result 
	retargeting_by_time: re-targeting user view history time to live

The range coefficients for shpinx, 0- off, 1-max:
	range/short_term: short term hostory
	range/ long_term: long term history
	range/context: page context
	range/search: user search keywords

The source database
	 {$db} in main,log
	main – the main database(read only), 
	log – log store

	mongo/{$db}/host: host:port
	mongo/{$db}/db: the database name
	mongo/{$db}/set: Specifies the name of the replica set, if the mongod is a member of a replica set
       	mongo/{$db}/slave: to use slaves data

Redis servers
	 {$db} in  short_term_history, long_term_history, user_view_history
	short_term_history – short term history, 
	long_term_history – long term history
	user_view_history  – user view history

	redis/{$db}/host: host
	redis/{$db}/port: port

Context search by offers:
Sphinx server:
    sphinx/host: host(ip address)
    sphinx/port: port
    sphinx/index: index name
    sphinx/mach_mode:
	SPH_MATCH_ALL Match all query words (default mode).
	SPH_MATCH_ANY Match any of query words.
	SPH_MATCH_PHRASE Match query as a phrase, requiring perfect match.
	SPH_MATCH_BOOLEAN Match query as a boolean expression.
	SPH_MATCH_EXTENDED Match query as an expression in Sphinx internal query language.
	SPH_MATCH_FULLSCAN Enables fullscan.
	SPH_MATCH_EXTENDED2 The same as SPH_MATCH_EXTENDED plus ranking and quorum searching support.

 sphinx/rank_mode:
	SPH_RANK_PROXIMITY_BM25 = sum(lcs*user_weight)*1000+bm25
	SPH_RANK_BM25 = bm25
	SPH_RANK_NONE = 1
	SPH_RANK_WORDCOUNT = sum(hit_count*user_weight)
	SPH_RANK_PROXIMITY = sum(lcs*user_weight)
	SPH_RANK_MATCHANY = sum((word_count+(lcs-1)*max_lcs)*user_weight)
	SPH_RANK_FIELDMASK = field_mask
	SPH_RANK_SPH04 = sum((4*lcs+2*(min_hit_pos==1)+exact_hit)*user_weight)*1000+bm25

 sphinx/sort_mode:
	SPH_SORT_RELEVANCE Sort by relevance in descending order (best matches first).
	SPH_SORT_ATTR_DESC Sort by an attribute in descending order (bigger attribute values first).
	SPH_SORT_ATTR_ASC Sort by an attribute in ascending order (smaller attribute values first).
	SPH_SORT_TIME_SEGMENTS Sort by time segments (last hour/day/week/month) in descending order, and then by relevance in descending order.
	SPH_SORT_EXTENDED Sort by SQL-like combination of columns in ASC/DESC order.
	SPH_SORT_EXPR Sort by an arithmetic expression.

 sphinx/fields{title,description,keywords,exactly_phrases,phrases,minuswords}: search fields with weight

Server configuration:
	server/socket_path: unix socket path fast cgi, the same path must in nginx.conf:
	        upstream fastcgi_backend {
              	  	server unix:/tmp/workerd.sock;
                	server unix:/tmp/workerd1.sock;
                	server unix:/tmp/workerd2.sock;
        		}
    	server/children: threads number
	server/mq_path: message queue server user and password
	server/dbpath sqlite database file path
	server/db_dump_path sqlite database dump(without data, only schema)
 	server/db_geo_csv geo ip database file path
 	server/template_teaser teaser template file path:
	        	%1%	    CSS
 %2%	    JSON
 %3%	    offers count
 %4%	    next link
	server/template_banner the banner template path
	server/swfobject swfobject.js file
 	server/geoGity geo city file path
	server/cookie_name coockie name
	server/cookie_domain: coockie domain name
	server/cookie_path coockie path
	server/lock_file lock file path
	server/pid_file pid file path
	server/user: run as user
	server/time_update: campaign by time update interval

 install:
    	#aclocal -I m4 && autoconf && automake
    	#./configure
 	#make

good luck:)
